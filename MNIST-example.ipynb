{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Partial Graph Evaluation\n",
    "\n",
    "This example illustrates the use of `partialflow` for training a neural network with heavy memory consumption on a GPU with limited memory resources. To keep things simple, we will train a convolutional network on MNIST and use a very large batch size to make the training process memory-intensive.\n",
    "\n",
    "First we prepare the MNIST dataset and build a tensorflow input queue with a batch size of 7500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "train_images = np.reshape(mnist.train.images, [-1, 28, 28, 1])\n",
    "train_labels = mnist.train.labels\n",
    "\n",
    "test_images = np.reshape(mnist.test.images, [-1, 28, 28, 1])\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "# training input queue with large batch size\n",
    "batch_size = 7500\n",
    "image, label = tf.train.slice_input_producer([train_images, train_labels])\n",
    "image_batch, label_batch = tf.train.batch([image, label], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Residual Network\n",
    "\n",
    "`partialflow` allows us to split a `tensorflow` graph into several sections which can then be trained separately to lower the memory consumption. This means that the training graph of each section on its own has to fit into GPU memory, whereas the full network's training graph may not.\n",
    "\n",
    "The graph sections are managed by a `GraphSectionManager` that orchestrates the data flow between the graph sections during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from partialflow import GraphSectionManager\n",
    "\n",
    "sm = GraphSectionManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `GraphSectionManager` to create new sections in which we define our network. `partialflow` automatically analyzes the `tensorflow` graph and keeps track of tensors flowing across section borders and variables defined in sections.\n",
    "\n",
    "In the following, we define our CNN in four sections. This is mainly done for illustrative purposes since two sections might already suffice, depending on your GPU memory. We added some `tf.Print` statements to make `tensorflow` log forward passes for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BasicNets import BatchnormNet\n",
    "\n",
    "# flag for batch normalization layers\n",
    "is_training = tf.placeholder(name='is_training', shape=[], dtype=tf.bool)\n",
    "net = BatchnormNet(is_training, image_batch)\n",
    "\n",
    "# first network section with initial convolution and three residual blocks\n",
    "with sm.new_section() as sec0:\n",
    "    with tf.variable_scope('initial_conv'):\n",
    "        stream = net.add_conv(net._inputs, n_filters=16)\n",
    "        stream = tf.Print(stream, [stream], 'Forward pass over section 0')\n",
    "        stream = net.add_bn(stream)\n",
    "        stream = tf.nn.relu(stream)\n",
    "    \n",
    "    with tf.variable_scope('scale0'):\n",
    "        for i in range(3):\n",
    "            with tf.variable_scope('block_%d' % i):\n",
    "                stream = net.res_block(stream)\n",
    "\n",
    "                \n",
    "# second network section strided convolution to decrease the input resolution\n",
    "with sm.new_section() as sec1:\n",
    "    with tf.variable_scope('scale1'):\n",
    "        stream = tf.Print(stream, [stream], 'Forward pass over section 1')\n",
    "        stream = net.res_block(stream, filters_factor=2, first_stride=2)\n",
    "        for i in range(2):\n",
    "            with tf.variable_scope('block_%d' % i):\n",
    "                stream = net.res_block(stream)\n",
    "\n",
    "# third network section\n",
    "with sm.new_section() as sec2:\n",
    "    with tf.variable_scope('scale2'):\n",
    "        stream = tf.Print(stream, [stream], 'Forward pass over section 2')\n",
    "        stream = net.res_block(stream, filters_factor=2, first_stride=2)\n",
    "        for i in range(4):\n",
    "            with tf.variable_scope('block_%d' % i):\n",
    "                stream = net.res_block(stream)\n",
    "        \n",
    "# fourth network section with final pooling and cross-entropy loss\n",
    "with sm.new_section() as sec3:\n",
    "    with tf.variable_scope('final_pool'):\n",
    "        stream = tf.Print(stream, [stream], 'Forward pass over section 3')\n",
    "        # global average pooling over image dimensions\n",
    "        stream = tf.reduce_mean(stream, axis=2)\n",
    "        stream = tf.reduce_mean(stream, axis=1)\n",
    "        \n",
    "        # final conv for classification\n",
    "        stream = net.add_fc(stream, out_dims=10)\n",
    "    \n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(stream, label_batch)\n",
    "        loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **the loss is defined inside a graph section**. This is necessary to ensure that the image and label batches are cached and reused during forward and backward passes over the network. If the loss were defined outside a section, the input queues might be evaluated multiple times which leads to incorrect gradients being propagated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add training operations and prepare training\n",
    "\n",
    "In order to construct the training graph for our network, we ask the `GraphSectionManager` to create training operations for each section. This can be done automatically as shown here, or by handing it a list of (possibly preprocessed) gradients as returned by `opt.compute_gradients`.\n",
    "\n",
    "The `verbose` parameter lets the manager add `tf.Print` statements into the gradient computation in order to log backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "\n",
    "sm.add_training_ops(opt, loss, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `GraphSectionManager` needs to analyze the data flows in forward and backward passes across the graph sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm.prepare_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we may perform some sanity checks to vaildate that the right tensors are cached and fed into training runs of different sections. For example, we expect the backward pass of section `sec2` to depend on some output of `sec1` as well as gradients computed in `sec3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'gradients/graph_section_3/final_pool/Mean_grad/truediv:0' shape=(7500, 7, 7, 64) dtype=float32>,\n",
       " <tf.Tensor 'graph_section_1/scale1/block_1/add:0' shape=(7500, 14, 14, 32) dtype=float32>,\n",
       " <tf.Tensor 'is_training:0' shape=() dtype=bool>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec2.get_tensors_to_feed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, the corresponding gradient tensors have to be cached during the backward pass of `sec3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'gradients/graph_section_3/final_pool/Mean_grad/truediv:0' shape=(7500, 7, 7, 64) dtype=float32>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec3.get_tensors_to_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Forward and Backward Passes\n",
    "\n",
    "We can now open a new session and initialize our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "_ = tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, a simple forward pass ignoring the sections can be performed using `session.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.650328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(loss, feed_dict={is_training: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should log a single forward pass for each section into the console running this notebook:\n",
    "\n",
    "```\n",
    "Forward pass over section 0[[[[0 0 0]]]...]\n",
    "Forward pass over section 1[[[[0.25482464 0.54249996 0.15713426]]]...]\n",
    "Forward pass over section 2[[[[1.3474643 0.62452459 0.14982516]]]...]\n",
    "Forward pass over section 3[[[[0.52292633 -0.39113081 0.74775648]]]...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Since intermediate results need to be cached for the backward pass, training operations need to be run using `GraphSectionManager`. The `run_full_cycle` method will run a forward pass, cache intermediate results as needed, and perform a backward pass over the training operations. \n",
    "\n",
    "Forward passes are not performed section-wise, because `tensorflow` optimizes memory consumption by dropping intermediate results anyway. Hence the full forward pass graph is assumed to fit into GPU memory. In contrast, backward passes are performed section-wise. `run_full_cycle` takes care of evaluating the graph elements in `fetches` during the right phases of this procedure.\n",
    "\n",
    "The following should log a full forward pass followed by interleaved forward and backward passes for each section. Note that the `basic_feed` parameter is used analogous to `feed_dict` in `session.run`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.725137"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.run_full_cycle(sess, fetches=loss, basic_feed={is_training:True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which should log something like\n",
    "\n",
    "```\n",
    "Forward pass over section 0[[[[0 0 0]]]...]\n",
    "Forward pass over section 1[[[[0.25558376 0.54339874 0.15626775]]]...]\n",
    "Forward pass over section 2[[[[1.3982055 0.59655607 0.18760961]]]...]\n",
    "Forward pass over section 3[[[[1.2530568 -1.3083258 -0.73674989]]]...]\n",
    "Running backward pass on section 3[-0.099787384 -0.11186664 -0.0903545...]\n",
    "Forward pass over section 2[[[[1.3982055 0.59655607 0.18760961]]]...]\n",
    "Running backward pass on section 2[[[[-0.55458224 0.38391361 -0.357202]]]...]\n",
    "Forward pass over section 1[[[[0.25558376 0.54339874 0.15626775]]]...]\n",
    "Running backward pass on section 1[[[[-0.0038170468 0.001159993 0.0018510161]]]...]\n",
    "Forward pass over section 0[[[[0 0 0]]]...]\n",
    "Running backward pass on section 0[-0.098705873 0.026503615 -0.0099251084...]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
